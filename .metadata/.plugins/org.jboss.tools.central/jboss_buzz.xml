<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How Ansible simplifies JBoss EAP deployment on Azure</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" /><author><name>Harsha Cherukuri</name></author><id>508c1342-d157-4be8-9332-a27590fde406</id><updated>2022-08-17T07:00:00Z</updated><published>2022-08-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to deploy &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform (JBoss EAP)&lt;/a&gt; on Microsoft Azure using &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; automation. Currently, Red Hat has a &lt;a href="https://azuremarketplace.microsoft.com/marketplace/apps/redhat.jboss-eap-rhel"&gt;Microsoft Azure Marketplace offering of JBoss EAP&lt;/a&gt;, but it's available only through the &lt;a href="https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/redhat/byos"&gt;Bring-Your-Own-Subscription (BYOS)&lt;/a&gt; model, which is relatively complex. This article creates Azure resources using &lt;a href="https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.html"&gt;Ansible Collections for Azure&lt;/a&gt; and then deploys JBoss EAP using the &lt;a href="https://github.com/ansible-middleware/wildfly"&gt;WildFly&lt;/a&gt; service provided by the &lt;a href="https://ansiblemiddleware.com/"&gt;Ansible Middleware project&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;&lt;strong&gt;Deploy JBoss EAP in 6 easy steps&lt;/strong&gt;&lt;/h2&gt; &lt;p&gt;We will use &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; as the sample application for this article. Using this application, we will automate and deploy JBoss EAP instances on Azure virtual machines running &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Step 1.  Prerequisites setup&lt;/h3&gt; &lt;p&gt;To run the sample application, please put the following requirements in place:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An Azure account with an active subscription. If you do not have an Azure subscription, &lt;a href="https://azure.microsoft.com/pricing/free-trial"&gt;create one for free&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;JBoss EAP: You need a Red Hat account with a Red Hat Subscription Management entitlement for JBoss EAP. This entitlement allows you to download a version of JBoss EAP tested and certified by Red Hat. If you do not have a JBoss EAP entitlement, sign up for a free developer subscription:  &lt;a href="https://developers.redhat.com/register"&gt;Red Hat Developer Subscription for Individuals&lt;/a&gt;. Once registered, you can find the necessary credentials (pool IDs) at the &lt;a href="https://access.redhat.com/management/"&gt;Red Hat customer portal&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The following software on the controller host or local machine: &lt;ul&gt; &lt;li&gt;Ansible (version 2.9 or greater)&lt;/li&gt; &lt;li&gt;Python (version 3.9 or greater)&lt;/li&gt; &lt;li&gt;Python3 &lt;a href="https://pypi.org/project/netaddr/"&gt;netaddr&lt;/a&gt; (obtained using dnf or pip)&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"&gt;Azure command-line interface&lt;/a&gt; (CLI)&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo application&lt;/a&gt; to your local machine.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Step 2.  Install WildFly and other components&lt;/h3&gt; &lt;p&gt;After you unpack the &lt;code&gt;azure-eap-demo&lt;/code&gt; application, change into the repository's top-level directory, and run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install -r requirements.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We are using a dynamic inventory provided by Azure. Once the instances are created, you can view the inventory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-inventory -i inventory/myazure_rm.yml --graph&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3.  Create credentials for Red Hat portal access&lt;/h3&gt; &lt;p&gt;You need to provide credentials in the playbook so that it can download software from the Red Hat customer portal. Specify your Red Hat account name in the &lt;code&gt;rhn_username&lt;/code&gt; variable and your password in the &lt;code&gt;rhn_password&lt;/code&gt; variable.&lt;/p&gt; &lt;p&gt;In addition, you have to specify the Red Hat Subscription Management entitlement for JBoss EAP in the &lt;code&gt;jboss_eap_rhn_id&lt;/code&gt; variable. This variable allows you to specify which version of JBoss EAP (supported by Red Hat) you would like to install. Alternatively, you can just download and install the JBoss EAP ZIP file from the Red Hat customer portal.&lt;/p&gt; &lt;p&gt;All these variables can be stored in a YAML file, whose name you specify in a section in the Ansible playbook named &lt;code&gt;vars_files&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 4.  Run the Ansible playbook&lt;/h3&gt; &lt;p&gt;Now run the Ansible playbook in &lt;code&gt;create-demo-setup.yml &lt;/code&gt;which creates resources on Azure and deploys JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;​​​​​​​$ ansible-playbook -e @rhn-creds.yml -i inventory/myazure_rm.yml -e "ansible_ssh_user=rheluser ansible_ssh_private_key_file='provide_your_ssh_private_key' hosts_group_name=eap wildfly_version=7.4 override_install_name=jboss-eap" create-demo-setup.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As part of the playbook execution, the &lt;a href="https://github.com/ansible-middleware/azure-eap-demo"&gt;azure-eap-demo&lt;/a&gt; repository is cloned. Its &lt;code&gt;create-demo-setup.yml&lt;/code&gt; file contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost gather_facts: false connection: local vars: repo_url: "https://github.com/ansible-middleware/wildfly-cluster-demo.git" branch: main tasks: - name: Git checkout ansible.builtin.git: repo: "{{ repo_url }}" dest: "{{ playbook_dir }}/wildfly-cluster-demo" version: "{{ branch }}" single_branch: yes clone: yes update: yes - name: Create demo resources on azure. include_role: name: 'azure' vars: ssh_key_path: "{{ ssh_key | default(lookup('env', 'HOME') + '/.ssh/id_rsa.pub')}}" - meta: refresh_inventory - pause: minutes: 1 - name: Run wildfly-cluster-demo import_playbook: wildfly-cluster-demo/playbook.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This playbook creates the Azure resources the application needs, including a resource group, virtual networks, subnets, security groups, network interfaces, three virtual machines running Red Hat Enterprise Linux, and public IP addresses for the virtual machines.&lt;/p&gt; &lt;p&gt;All the default parameters for the Azure cloud instances are within the installed package: &lt;code&gt;roles/azure/defaults/main.yml&lt;/code&gt; ​​​​​​.&lt;/p&gt; &lt;p&gt;Finally, the playbook deploys the WildFly cluster demo. Refer to the article &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; to learn more about how to use WildFly.&lt;/p&gt; &lt;h3&gt;Step 5.  Verify deployment of the JBoss EAP cluster and application&lt;/h3&gt; &lt;p&gt;Once the playbook completes successfully, you can verify the JBoss EAP cluster by logging into the &lt;a href="https://portal.azure.com/"&gt;Azure portal&lt;/a&gt;. Here, you will find all the resources created to support the JBoss EAP cluster. Log in or SSH into any of the virtual machines created and confirm that the WildFly service is running and accessible. Alternatively, you can run the &lt;code&gt;validate.yml&lt;/code&gt; playbook provided in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;wildfly-cluster-demo&lt;/a&gt; application to validate the configuration.&lt;/p&gt; &lt;h3&gt;Step 6.  Clean up Azure resources&lt;/h3&gt; &lt;p&gt;To clean up all the resources created on Azure, run the &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-playbook clean-demo-resources.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The contents of &lt;code&gt;clean-demo-resources.yml&lt;/code&gt; are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: Create Azure VM hosts: localhost connection: local tasks: - name: Create VM's on azure. include_role: name: 'azure' vars: action: destroy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;destroy&lt;/code&gt; action runs the &lt;code&gt;roles/azure/tasks/destroy.yml&lt;/code&gt; file, which contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: Remove a VM and all resources that were autocreated azure_rm_virtualmachine: resource_group: "{{ item.resourcegroup_name }}" name: "{{ item.name }}" remove_on_absent: all_autocreated state: absent loop: "{{ vm }}" - name: Delete a resource group including resources it contains azure_rm_resourcegroup: name: "{{ item.name }}" force_delete_nonempty: yes state: absent loop: "{{ resource_groups }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook removes all the virtual machines and deletes all the resources under the &lt;code&gt;eap-cluster&lt;/code&gt; resource group.&lt;/p&gt; &lt;h2&gt;Ansible simplifies deployment on Azure&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated a step-by-step process to create resources using Ansible on Microsoft Azure and deploy a JBoss EAP cluster using tooling from the Ansible Middleware project. Check out the other collections and demos within the &lt;a href="https://github.com/ansible-middleware"&gt;ansible-middleware&lt;/a&gt; GitHub organization and the &lt;a href="https://ansiblemiddleware.com"&gt;Ansible Middleware website&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/17/how-ansible-simplifies-jboss-eap-deployment-azure" title="How Ansible simplifies JBoss EAP deployment on Azure "&gt;How Ansible simplifies JBoss EAP deployment on Azure &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Harsha Cherukuri</dc:creator><dc:date>2022-08-17T07:00:00Z</dc:date></entry><entry><title type="html">Getting started with AtlasMap</title><link rel="alternate" href="http://www.mastertheboss.com/java/getting-started-with-atlasmap/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/getting-started-with-atlasmap/</id><updated>2022-08-16T16:39:32Z</updated><content type="html">This article is a whirlwind tour of AtlasMap Data transformation API and User Interface. We will learn how to use its editor to define mapping rules and how to use them in a sample Java Integration project. Data transformation is the process of converting data from one format or structure into another format or structure. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Connect MongoDB to a Node.js application with kube-service-bindings</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" /><author><name>Costas Papastathis, Michael Dawson</name></author><id>75fcba18-2deb-4b89-b6c2-c6e2fc0932a6</id><updated>2022-08-16T07:00:01Z</updated><published>2022-08-16T07:00:01Z</published><summary type="html">&lt;p&gt;This is the third and final article in a three-part series introducing &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; for &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developers on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Together with the &lt;a href="https://operatorhub.io/operator/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; (SBO), kube-service-bindings makes it easier to share credentials for services with your applications.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/06/29/connect-services-kubernetes-easily-kube-service-bindings"&gt;first article of this series&lt;/a&gt; offered background on the tools we're using, and the &lt;a href="https://developers.redhat.com/articles/2022/07/22/enable-backing-services-kubernetes-service-service-binding-operator-and-kube"&gt;second&lt;/a&gt; set up some basic elements such as hosting on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and a &lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt; database. Now we're going to use all these tools to create a binding between our Node.js application and the database.&lt;/p&gt; &lt;h2&gt;Deploy MongoDB as a cloud-hosted database&lt;/h2&gt; &lt;p&gt;The previous article set up access between &lt;a href="https://www.mongodb.com/cloud/atlas/lp/try7?utm_source=google&amp;utm_campaign=gs_americas_united_states_search_core_brand_atlas_desktop&amp;utm_term=mongodb%20atlas&amp;utm_medium=cpc_paid_search&amp;utm_ad=e&amp;utm_ad_campaign_id=12212624338&amp;adgroup=115749704063&amp;gclid=Cj0KCQjw8uOWBhDXARIsAOxKJ2Hyk1VN_iVHDUUBWqUwwxLeVOw2WtbigZMayXJdUmnptIUnJUod8xAaAs1MEALw_wcB"&gt;MongoDB Atlas&lt;/a&gt; and your OpenShift cluster. If you went through those steps successfully, you are ready to deploy a cloud-hosted MongoDB database in the cluster as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the OpenShift console, visit the Topology view by selecting &lt;strong&gt;Developer→Topology&lt;/strong&gt; from the left sidebar.&lt;/li&gt; &lt;li&gt;Select the project where you would like to deploy the cloud database by selecting, from the top of the left sidebar, &lt;strong&gt;Developer&lt;/strong&gt;→&lt;strong&gt;Topology&lt;/strong&gt;→&lt;strong&gt;Project&lt;/strong&gt;. From the dropdown menu, select your project.&lt;/li&gt; &lt;li&gt;In the left sidebar menu, click &lt;strong&gt;+Add→Cloud-Hosted Database→MongoDB Atlas Cloud Database Service→Add to Topology&lt;/strong&gt;. Select your database instance and click &lt;strong&gt;Add to topology→Continue&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Upon successful connection, you are taken to the Topology view, where the cloud-hosted database is deployed and visible (Figure 1).&lt;/li&gt; &lt;/ol&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/mongo_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/mongo_0.png?itok=oJytAkSx" width="341" height="284" alt="The Topology view shows that MongoDB Atlas is now accessible in your cluster." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Topology view shows that MongoDB Atlas is now accessible in your cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Deploy the Node.js application in OpenShift&lt;/h2&gt; &lt;p&gt;There are several ways to deploy a Node.js application in OpenShift: Through the &lt;code&gt;oc&lt;/code&gt; OpenShift command-line interface (CLI), the &lt;code&gt;odo&lt;/code&gt; CLI, the OpenShift console, etc. This article covers two options: The OpenShift console and &lt;a href="https://www.npmjs.com/package/nodeshift"&gt;Nodeshift&lt;/a&gt;, an NPM package.&lt;/p&gt; &lt;h3&gt;Deploy through the OpenShift console&lt;/h3&gt; &lt;p&gt;From the &lt;strong&gt;Developer&lt;/strong&gt; perspective, select &lt;strong&gt;+Add→Import from Git&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In the &lt;strong&gt;Git Repo url&lt;/strong&gt; field, set the repository URL to &lt;code&gt;https://github.com/nodeshift-blog-examples/kube-service-bindings-examples&lt;/code&gt;. This is a &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;kube-service-bindings examples repository&lt;/a&gt; maintained by our team; it contains the Node.js application you are deploying in this article.&lt;/p&gt; &lt;p&gt;Expand &lt;strong&gt;Show advanced Git options&lt;/strong&gt;. On the &lt;strong&gt;Context dir&lt;/strong&gt; field, set the value to &lt;code&gt;src/mongodb&lt;/code&gt;, which is the path of the subdirectory where your Node.js application is located.&lt;/p&gt; &lt;p&gt;On &lt;strong&gt;Builder Image&lt;/strong&gt;, select &lt;strong&gt;Node.js&lt;/strong&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;h3&gt;Deploy through Nodeshift&lt;/h3&gt; &lt;p&gt;Open a terminal and clone the git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/nodeshift-blog-examples/kube-service-bindings-examples.git $ cd ./kube-service-bindings-examples/src/mongodb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install Nodeshift globally:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npm install -g nodeshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To find the login credentials required by the next command you'll use, visit your OpenShift console. In the upper right corner, click your username. A dropdown will appear. Click &lt;strong&gt;Copy login command&lt;/strong&gt; (Figure 2), which transfers you to another page. Then click &lt;strong&gt;Display Token&lt;/strong&gt; to display the username, password, and server credentials to log in with Nodeshift.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/copy_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/copy_1.png?itok=6SxN8H8y" width="378" height="209" alt="Under your name in the console, you can obtain login credentials." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Under your name in the console, you can obtain login credentials. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Using these credentials, you can now log in to your OpenShift cluster with Nodeshift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift login --username=developer --password=password --server=https://api.server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deploy the Node.js application with Nodeshift through the following command, replacing the namespace name with your specific project name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ nodeshift --namespace.name=&lt;selected-project&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your application should be deployed and visible in the Topology view, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/node.png?itok=CJxGzdXs" width="227" height="236" alt="The Node.js application appears in the Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Node.js application appears in the Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Establish a connection between the Node.js application and the MongoDB database&lt;/h2&gt; &lt;p&gt;The final step in this series is to establish a connection between your Node.js application and the MongoDB database, which we'll accomplish in this section.&lt;/p&gt; &lt;h3&gt;Service Binding Operator&lt;/h3&gt; &lt;p&gt;At this point, two instances should show up in your Topology view: the Node.js application and the connection to your MongoDB instance in Atlas (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/two_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/two_0.png?itok=uWDdr5gL" width="675" height="284" alt="The Topology view shows both the Node.js application and the external MongoDB database." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Topology view shows both the Node.js application and the external MongoDB database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To establish a connection between these instances, you will use the Service Binding Operator to share the credentials and kube-service-bindings to parse those credentials (binding data).&lt;/p&gt; &lt;p&gt;You can create a Service Binding in two different ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drag a line in the Topology view between the two backing services (the Node.js application and MongoDB).&lt;/li&gt; &lt;li&gt;Apply a YAML file specifying the service binding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will go with the first option, which in our case is faster and easier.&lt;/p&gt; &lt;p&gt;Hover the mouse over the Node.js application in the Topology view. An arrow should appear. Drag the arrow from the Node.js application to the circle around the MongoDB instance. A tooltip should be visible that says &lt;strong&gt;Create service binding&lt;/strong&gt;. Release the mouse button and a pop-up box will let you specify the name of the service binding. Click &lt;strong&gt;Create binding&lt;/strong&gt;. The container of the Node.js application will restart immediately (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Check the environment of Node.js application by clicking the Node.js application container in the Topology view. In the right sidebar, click &lt;strong&gt;Resources→View Logs (Pods Section)&lt;/strong&gt; and visit the &lt;strong&gt;Environment&lt;/strong&gt; tab. The &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable should be set, as shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;kube-service-bindings&lt;/h3&gt; &lt;p&gt;The final step is to read the binding data under the directory indicated by the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable and pass the data to the MongoDB client to establish a connection to the MongoDB database. Your Node.js application already has kube-service-bindings as a dependency. So calling the &lt;code&gt;getBinding()&lt;/code&gt; function, as shown in the following JavaScript code snippet, does all the hard work of parsing, cleaning, and transforming the binding data into a consumable format for the MongoDB client:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { MongoClient } = require("mongodb"); const serviceBindings = require("kube-service-bindings"); const { url, connectionOptions } = serviceBindings.getBinding("MONGODB", "mongodb"); const mongoClient = new MongoClient(url, connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it. By visiting the URL of the Node.js application (click the arrow-box icon on the node), you can perform simple CRUD operations through the UI on the database.&lt;/p&gt; &lt;h2&gt;Easier integration with services on Kubernetes&lt;/h2&gt; &lt;p&gt;Over the past year, our team has been active in developing kube-service-bindings, making it easier for developers with little or no experience in managing containerized applications to securely share credentials among backing services.&lt;/p&gt; &lt;p&gt;Complementing the work on &lt;a href="https://github.com/nodeshift/kube-service-bindings"&gt;kube-service-bindings development&lt;/a&gt;, our team provides &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;examples&lt;/a&gt; for most of the clients supported by kube-service-bindings, instructions on utilizing kube-service-bindings, and a description of how to deploy a variety of backing services through Nodeshift in Kubernetes and OpenShift environments.&lt;/p&gt; &lt;p&gt;This series of articles has shown which clients are supported and how both a service binding and kube-service-bindings work. We guided you through the whole cycle of deploying a Node.js application backing service using the SBO and kube-service-bindings, sharing and parsing credentials for a connection between a Node.js application and a MongoDB database. kube-service-bindings read, parsed, and transformed binding data projected by the Service Binding Operator, returning data in a form directly consumable by the MongoDB client.&lt;/p&gt; &lt;p&gt;To help you use kube-service-bindings in other types of deployments, we have provided additional &lt;a href="https://github.com/nodeshift-blog-examples/kube-service-bindings-examples"&gt;Node.js examples&lt;/a&gt;. We hope you found this article interesting and now have a better understanding of kube-service-bindings and service bindings in general.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings" title="Connect MongoDB to a Node.js application with kube-service-bindings"&gt;Connect MongoDB to a Node.js application with kube-service-bindings&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Costas Papastathis, Michael Dawson</dc:creator><dc:date>2022-08-16T07:00:01Z</dc:date></entry><entry><title>How to set up Packit to simplify upstream project integration</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" /><author><name>Laura Barcziova</name></author><id>e727f048-6620-4039-9cca-83550b9309cc</id><updated>2022-08-16T07:00:00Z</updated><published>2022-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;If you use open source projects from GitHub or GitLab in your infrastructure, you have probably established workflows to create project builds that you can install. You might create builds regularly and execute them in a similar manner across projects. Let's say you want to build the code changes in &lt;a href="https://getfedora.org"&gt;Fedora Linux&lt;/a&gt; or &lt;a href="https://www.centos.org/centos-stream/"&gt;CentOS Stream&lt;/a&gt; for each commit, release, or pull request in these projects. &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt; can automate this policy, fold into a &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; pipeline, and do even more.&lt;/p&gt; &lt;p&gt;Packit is an open source project that tests and builds RPM packages on Fedora Linux, CentOS Stream, and other distributions to ease the integration of upstream projects with the distributions.&lt;/p&gt; &lt;p&gt;This article focuses on &lt;a href="https://packit.dev/docs/packit-service/"&gt;Packit Service&lt;/a&gt;, which operates on GitHub and GitLab. You can also install a &lt;a href="https://packit.dev/docs/cli/"&gt;command line interface (CLI)&lt;/a&gt; locally to run Packit on your desktop or laptop.&lt;/p&gt; &lt;h2&gt;Setting up Packit in 3 steps&lt;/h2&gt; &lt;p&gt;Setting up Packit is pretty straightforward. Follow these three steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a valid &lt;a href="https://fedoraproject.org/wiki/Account_System"&gt;Fedora Account System &lt;/a&gt;account (if you don't already have one).&lt;/li&gt; &lt;li&gt;Install our GitHub application on &lt;a href="https://github.com/marketplace/packit-as-a-service"&gt;GitHub Marketplace&lt;/a&gt;, or &lt;a href="https://packit.dev/docs/guide/#how-to-set-up-packit-on-gitlab"&gt;configure a webhook&lt;/a&gt; on GitLab (depending on where your project lives).&lt;/li&gt; &lt;li&gt;Provide your FAS username, which will be verified (on Github, the verification is automatic).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's all! Now you can add a configuration file to your project's repository and start setting up &lt;a href="https://packit.dev/docs/configuration/#packit-service-jobs"&gt;Packit service jobs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Most jobs require an &lt;a href="https://rpm-packaging-guide.github.io/#what-is-a-spec-file"&gt;RPM spec file&lt;/a&gt;. You can either place the spec file directly in your upstream repository or tell Packit how to download it from somewhere else.&lt;/p&gt; &lt;h2&gt;Packit for continuous integration&lt;/h2&gt; &lt;p&gt;Let's start with a simple example. You need to determine if the new code changes in GitHub or GitLab project built on all stable Fedora distros and CentOS Stream 9 will be successful. The process for such verifications is to get RPM builds for each pull request in your upstream project. The configuration file should include a build job like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;- job: copr_build trigger: pull_request targets: - fedora-stable-x86_64 - centos-stream-9-x86_64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Easy, right? So what does this configuration accomplish? Packit takes the code changes for each pull request action, submits a new build in the &lt;a href="https://copr.fedorainfracloud.org/"&gt;Copr build system&lt;/a&gt;, and reports the results in the pull requests, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_10.png?itok=v9mQdDx1" width="925" height="347" alt="Packit creates commit checks that show results of the builds with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Packit creates commit checks that show results of the builds with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;You also have access to the actual Copr repository containing the builds. This access may be beneficial if you set up Packit to react to commits or releases because you can provide your RPM builds via the Copr repository to enable anyone to install them. You can also use Packit to do the builds in your Copr repository by specifying the owner and project name. Figure 2 shows one such repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image5_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image5_1.png?itok=BqZXrwOc" width="1161" height="564" alt="A Copr repository shows the latest builds for Cockpit projects." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A Copr repository shows the latest builds for Cockpit projects. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Figure 3 shows a build by Packit in the repository.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image4_2.png?itok=VxBj-cem" width="809" height="498" alt="A Packit build displays general information about a Cockpit project build in the Copr repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. A Packit build displays general information about a Cockpit project build in the Copr repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Also, if you package your project in Fedora, you will be able to run scratch builds in a similar manner directly in &lt;a href="https://koji.fedoraproject.org"&gt;Koji&lt;/a&gt;, the Fedora build system.&lt;/p&gt; &lt;p&gt;Apart from the RPM builds, you can set up Packit to run your tests in the &lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; infrastructure. The tests can either use the built RPMs from Copr or run independently. You can read more information about tests in &lt;a href="https://packit.dev/docs/testing-farm/"&gt;the Packit documentation&lt;/a&gt;. Figure 4 shows how Packit reports test results.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image3_4.png?itok=K4ciQ_LS" width="914" height="347" alt="Packit creates commit checks that show results of the tests with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Packit creates commit checks that show results of the tests with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Packit automation for Fedora downstream releases&lt;/h2&gt; &lt;p&gt;If you have ever maintained a package in Fedora, you probably know that releasing a new version downstream can be tedious. Packit can help you with boring tasks and do the repetitive work for you. With Packit, you can easily get your upstream releases into &lt;a href="https://src.fedoraproject.org"&gt;Fedora Package Sources&lt;/a&gt;, automatically submit builds in the &lt;a href="https://koji.fedoraproject.org/koji/"&gt;Koji build system&lt;/a&gt;, and create &lt;a href="https://bodhi.fedoraproject.org"&gt;Bodhi updates&lt;/a&gt;. If you are interested in this kind of automation, make sure to check our &lt;a href="https://packit.dev/docs/fedora-releases-guide/"&gt;release guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Ready to give Packit a try?&lt;/h2&gt; &lt;p&gt;If you are dealing with any of the situations mentioned, please check &lt;a href="https://packit.dev/docs/"&gt;our documentation&lt;/a&gt;, which will guide you through the Packit setup. If you have any questions, feel free to &lt;a href="https://packit.dev/#contact"&gt;contact us&lt;/a&gt;. We are happy to help and receive feedback or suggestions for features you would like to add.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration" title="How to set up Packit to simplify upstream project integration"&gt;How to set up Packit to simplify upstream project integration&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Laura Barcziova</dc:creator><dc:date>2022-08-16T07:00:00Z</dc:date></entry><entry><title>How OpenShift Serverless Logic evolved to improve workflows</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" /><author><name>Daniel Oh, Simon Seagrave</name></author><id>8a532101-1b3e-4c4f-a8f4-e08ff8c689fe</id><updated>2022-08-15T07:00:00Z</updated><published>2022-08-15T07:00:00Z</published><summary type="html">&lt;p&gt;Serverless is an advanced cloud deployment model that aims to run business services on demand, enabling enterprises to save infrastructure costs tremendously. The benefit of serverless is an application designed and developed as abstract functions regardless of programming languages. This article describes how the serverless and function models have evolved since they were unleashed upon the world with AWS Lambda and what to look forward to with Red Hat OpenShift serverless logic.&lt;/p&gt; &lt;h2&gt;The 3 phases of serverless evolution&lt;/h2&gt; &lt;p&gt;As serverless technologies evolve, we at Red Hat created the evolutionary scale to help our customers better understand how serverless has grown and matured over time. The three phases of serverless evolution are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.0 &lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;At the beginning of the serverless era, the 1.0 phase, serverless was thought of as functions with tiny snippets of code running on demand for a short period. AWS Lambda made this paradigm popular, but it had limitations in terms of execution time, protocols, and runtimes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 1.5&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;With the increase in popularity of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;&lt;u&gt;Kubernetes&lt;/u&gt;&lt;/a&gt; running microservices on container platforms, the serverless era also moved forward to the 1.5 phase. This phase augmented serverless traits and benefits by deploying polyglot runtimes and container-based functions. The serverless 1.5 phase also delivered an abstraction layer to manage serverless applications using the open Kubernetes serverless community project, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;&lt;u&gt;Knative&lt;/u&gt;&lt;/a&gt;. Red Hat was one of the founding members of the Knative project and continues to be one of the top contributors to that community. But this was not the end of the journey.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Serverless 2.0&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;We are now approaching the new serverless 2.0 phase. This phase involves more complex orchestration and integration patterns combined with some level of state management. Serverless functions are often thought of as stateless applications. But serverless workflows are designed for complex orchestrations of multiple services and functions, typically while preserving the state. The adoption of serverless increases as organizations perform more complex orchestrations. Consequently, the OpenShift Serverless team implements serverless workflows, utilizing our command of the business process automation space.&lt;/p&gt; &lt;h2&gt;The future of serverless workflows&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://serverlessworkflow.io/"&gt;&lt;u&gt;Serverless Workflow&lt;/u&gt;&lt;/a&gt; project is an open source specification that enables developers to design workflows running serverless functions using a standard domain-specific language (DSL). For increased flexibility, Serverless Workflows also allow developers to define business logic triggered by multiple events and services such as &lt;a href="https://cloudevents.io/"&gt;&lt;u&gt;CloudEvents&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://swagger.io/specification/"&gt;&lt;u&gt;OpenAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;&lt;u&gt;AsyncAPI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://graphql.org/"&gt;&lt;u&gt;GraphQL&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://grpc.io/"&gt;&lt;u&gt;gRPC&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Red Hat is one of the &lt;a href="https://serverlessworkflow.io/community.html"&gt;&lt;u&gt;project maintainers&lt;/u&gt;&lt;/a&gt; of the Serverless Workflow project. We have been actively involved in innovation and contribution since the earliest days of the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; project.&lt;/p&gt; &lt;p&gt;We are about to release a new feature of OpenShift Serverless called the serverless logic in developer preview. This feature allows developers to design workflows with serverless deployment and function development capabilities based on Knative and &lt;a href="https://kogito.kie.org/"&gt;&lt;u&gt;Kogito.&lt;/u&gt;&lt;/a&gt; Kogito augments function orchestration and automation to implement serverless workflows at scale on Red Hat OpenShift.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Stay tuned to the official &lt;a href="https://twitter.com/rhdevelopers"&gt;&lt;u&gt;@rhdevelopers Twitter stream&lt;/u&gt;&lt;/a&gt; and this blog for more details about our exciting new capability as we approach the release of Red Hat OpenShift Serverless Logic. We will also provide tutorials.&lt;/p&gt; &lt;p&gt;In the meantime, learn more about &lt;a href="https://developers.redhat.com/topics/serverless-java"&gt;OpenShift Serverless&lt;/a&gt;, and try it out by setting up your free and easy-to-use &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat Sandbox&lt;/a&gt; environment.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/15/how-openshift-serverless-logic-evolved-improve-workflows" title="How OpenShift Serverless Logic evolved to improve workflows"&gt;How OpenShift Serverless Logic evolved to improve workflows&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh, Simon Seagrave</dc:creator><dc:date>2022-08-15T07:00:00Z</dc:date></entry><entry><title type="html">First Steps with Dapr</title><link rel="alternate" href="http://www.ofbizian.com/2022/08/first-steps-with-dapr.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/08/first-steps-with-dapr.html</id><updated>2022-08-13T10:33:00Z</updated><content type="html">I recently to and work on the Dapr project. I about Dapr when it was initially announced by Microsoft, but hadn’t looked into it since it CNCF. Two years later, during my onboarding into the new role, I spent some time looking into it and here are the steps I took in the journey and my impressions so far. WHAT IS DAPR? TL;DR: Dapr is a distributed systems toolkit in a box. It addresses the peripheral integration concerns of applications and lets developers focus on the business logic. If you are familiar with Apache Camel, Spring Framework in the Java world, or other distributed systems frameworks, you will find a lot of similarities with Dapr. Here are a few parallels with other frameworks: * Similar to Camel, Dapr has connectors (called ) that let you connect to various external systems. * Similar to HashiCorp Consul, Dapr offers which can be backed by Consul. * Similar to Spring Integration, Spring Cloud, (remember Netflix Hystrix?) and many other frameworks, Dapr has error handling capabilities with retries, timeouts, circuit breakers which are called . * Similar to Spring Data KeyValue, Dapr offers Key/Value-based state abstractions. * Similar to Kafka, Dapr offers pub/sub-based service interactions. * Similar to ActiveMQ clients, Dapr offers , but these are not specific to a messaging technology, which means they can be used even with things such as AWS SQS or Redis for example. * Similar to Spring Cloud Config, Dapr offers configuration and secret management * Similar to Zookeeper or Redis clients, Dapr offers * Similar to a Service Mesh, Dapr offers mTLS and between your application and the sidecar. * Similar to Envoy, Dapr offers enhanced through automatic metrics, tracing and log collection. The primary difference between all of these frameworks and Dapr is that the latter offers its capabilities not as a library within your application, but as a sidecar running next to your application. These capabilities are exposed behind well-defined HTTP and gRPC APIs (very creatively called ) where the implementations (called ) can be swapped w/o affecting your application code. High-level Dapr architecture You could say, Dapr is a collection of stable APIs exposed through a sidecar and swappable implementations running somewhere else. It is the cloudnative incarnation of integration technologies that makes integration capabilities previously available only in a few languages, available to everybody, and portable everywhere: Kubernetes, on-premise, or literally (I mean the edge). GETTING STARTED The project is surprisingly easy to get up and running regardless of your developer background and language of choice. I was able to follow the getting started guides and run various quickstarts in no time on my MacOS. Here are roughly the steps I followed. INSTALL DAPR CLI Dapr CLI is the main tool for performing Dapr-related tasks such as running an application with Dapr, seeing the logs, running Dapr dashboard, or deploying all to Kubernetes. brew install dapr/tap/dapr-cli With the CLI installed, we have a few different options for installing and running Dapr. I’ll start from the least demanding and flexible option and progress from there. OPTION 1: INSTALL DAPR WITHOUT DOCKER This is the lightest but not the most useful way to run Dapr. dapr init --slim In this only daprd and placement binaries are installed on the machine which is sufficient for running Dapr sidecars locally. Run a Dapr sidecar The following command will start a Dapr sidecar called no-app listening on HTTP port 3500 and a random gRPC port. dapr run --app-id no-app --dapr-http-port 3500 Congratulations, you have your first Dapr sidecar running. You can see the sidecar instance through this command: dapr list or query its health status: curl -i http://localhost:3500/v1.0/healthz Dapr sidecars are supposed to run next to an application and not on their own. Let’s stop this instance and run it with an application. dapr stop --app-id no-app Run a simple app with a Dapr sidecar For this demonstration we will use a simple NodeJS : git clone cd samples/hello-dapr-slim npm install This is a Hello World the Dapr way and here is the gist of it: app.post('/neworder', bodyParser.json(), (req, res) =&gt; { const data = req.body.data; const orderId = data.orderId; res.status(200).send("Got a new order! Order ID: " + orderId); }); The application has one /neworder endpoint listening on port 3000. We can run this application and the sidecar with the following command: dapr run --app-id nodeapp --app-port 3000 --dapr-http-port 3500 node app.js The command starts the NodeJS application on port 3000 and Dapr HTTP endpoint on 3500. Once you see in the logs that the app has started successfully, we can poke it. But rather than hitting the /neworder endpoint directly on port 3000, we will instead interact with the application through the sidecar. We do that using Dapr CLI like this: dapr invoke --verb POST --app-id nodeapp --method neworder --data '{"data": { "orderId": "41" } }' And see the response from the app. If you noticed, the CLI only needs the app-id (instead of host and port) to locate where the service is running. The CLI is just a handy way to interact with your service. It that seems like too much magic, we can use bare-bones curl command too: curl -XPOST -d @sample.json -H "Content-Type:application/json" http://localhost:3500/v1.0/invoke/nodeapp/method/neworder This command uses the service Dapr’s invocation API to synchronously interact with the application. Here is a visual representation of what just happened: Invoking an endpoint through Dapr sidecar Now, with Dapr on the request path, we get the Daprized service invocation benefits such as resiliency policies such as retries, timeouts, circuit breakers, concurrency control; observability enhancements such as: metrics, tracing, logs; security enhancements such as mTLS, , etc. At this point, you can try out metadata, metrics endpoints, play with the options, or see your single microservice in Dapr dashboard. dapr dashboard The slim mode we are running on is good for the Hello World scenario, but not the best setup for local development purposes as it lacks state store, pub/sub, metric server, etc. Let’s stop the nodeapp using the command from earlier (or CTL +C), and remove the slim Dapr binary: dapr uninstall One thing to keep in mind is that this command will not remove the default configuration and component specification files usually located in: ~/.dapr folder. We didn’t create any files in the steps so far, but if you follow other tutorials and change those files, they will remain and get applied with every dapr run command in the future (unless overridden). This caused me some confusion, keep it in mind. OPTION 2: INSTALL DAPR WITH DOCKER This is the preferred way for running for development purposes but it requires Docker. Let’s set it up: dapr init The command will download and run 3 containers * Dapr placement container used with actors(I wish this was an optional feature) * Zipkin for collecting tracing information from our sidecars * And a single node Redis container used for state store, pub/sub, distributed-lock implementations. You can verify when these containers are running and you are ready to go. docker ps RUN THE QUICKSTARTS My next step from here was to try out the that demonstrate the building blocks for service invocation, pub/sub, state store, bindings, etc. The awesome thing about these quickstarts is that they demonstrate the same example in multiple ways: * With Dapr SDK and w/o any dependency to Dapr SDK i.e. using HTTP only. * In multiple languages: Java, Javascript, .Net, Go, Python, etc. You can mix and match different languages and interaction methods (SDK or native) for the same example which demonstrates Dapr’s polyglot nature. Option 3: Install Dapr on Kubernetes If you have come this far, you should have a good high-level understanding of what Dapr can do for you. The next step would be to deploy where most of the Dapr functionalities are available and closest to a production deployment. For this purpose, I used minikube locally with default settings and no custom tuning. dapr init --kubernetes --wait If successful, this command will start the following pods in dapr-system namespace: * dapr-operator: manages all components for state store, pub/sub, configuration, etc * dapr-sidecar-injector: injects dapr sidecars into deployment pods * dapr-placement: required with actors only. * dapr-sentry: manages mTLS between services and acts as a certificate authority. * dapr-dashboard: a simple webapp to explore what is running within a Dapr cluster These Pods collectively represent the Dapr . Injecting a sidecar From here on, adding a Dapr sidecar to an application (this would be Dapr dataplane) is as easy as adding the following to your Kubernetes Deployments:  annotations:    dapr.io/enabled: "true"    dapr.io/app-id: "nodeapp"    dapr.io/app-port: "3000" The dapr-sidecar-injector service watches for new Pods with the dapr.io/enabled annotation and injects a container with the daprd process within the pod. It also adds DAPR_HTTP_PORT and DAPR_GRPC_PORT environment variables to your container so that it can easily communicate with Dapr without hard-coding Dapr port values. To deploy a complete application on Kubernetes I suggest this step-by-step . It has a provider and consumer services and it worked the first time for me. TRANSPARENT VS EXPLICIT PROXY Notice Dapr sidecar injection is less intrusive than a typical service mesh with a transparent sidecar such as Istio’s Envoy. To inject a transparent proxy, typically the Pods also get injected with an init-container that runs at the start of the Pod and re-configures the Pods networking rules so that all ingress and egress traffic or your application container goes through the sidecar. With Dapr, that is not the case. There is a sidecar injected, but your application is in control of when and how to interact with Dapr over its well-defined explicit (non-transparent) APIs. Transparent service mesh proxies operate at lower network layers typically used by operations teams, whereas Dapr provides application layer primitives needed by developers. If you are interested in this topic, is a good explanation of the differences and overlaps of Dapr with services meshes. SUMMARY And finally, here are some closing thoughts with what I so far liked more and what less from Dapr. LIKED MORE * I love the fact that Dapr is one of the few CNCF projects targeting developers creating applications, and not only operations team who are running these applications. We need more cloudnative tools for implementing applications. * I love the non-intrusive nature of Dapr where capabilities are exposed over clear APIs and not through some black magic. I prefer transparent actions for instrumentation, observability, and general application insight, but not for altering application behavior. * I loved the polyglot nature of Dapr offering its capabilities to all programming languages and runtimes. This is what attracted me to Kubernetes and cloudnative in the first place. * I loved how easy it is to get started with Dapr and the many permutations of each quickstart. There is something for everyone regardless of where you are coming from into Dapr. * I’m excited about WASM and remote components features coming into Dapr. These will open new surface areas for more contributions and integrations. LIKED LESS * I haven’t used actors before and it feels odd to have a specific programming model included in a generic distributed systems toolkit. Luckily you don’t have to use it if you don’t want to. * The documentation is organized, but too sparse into multiple short pages. Learning a topic will require navigating a lot of pages multiple times, and it is still hard to find what you are looking for. Follow me at to join my journey of learning and using Dapr and shout out with any thoughts and comments.</content><dc:creator>Unknown</dc:creator></entry><entry><title>Implement multitenant SaaS on Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/12/implement-multitenant-saas-kubernetes" /><author><name>Bob Reselman</name></author><id>5072fe6d-3baf-4cac-80c4-b119a34825b1</id><updated>2022-08-12T07:00:00Z</updated><published>2022-08-12T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the second in a series about implementing a multitenant, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; SaaS application. The first article, &lt;a href="https://developers.redhat.com/articles/2022/06/16/how-convert-web-application-software-service"&gt;How to convert a web application to Software-as-a-Service&lt;/a&gt;, discussed from a conceptual point of view how to convert a standalone web application into generic code that powers a SaaS platform. This article demonstrates in a concrete manner how to implement a multitenant SaaS in a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster.&lt;/p&gt; &lt;p&gt;The example in the previous article converted a fictitious standalone application named Clyde's Clarinets into a SaaS platform named Instrument Resellers. The purpose of Clyde's Clarinets was to acquire, refurbish, and resell used clarinets. Clyde's Clarinets evolved into the Instrument Resellers SaaS platform so that any business could acquire, refurbish, and resell a particular type of instrument. Thus, Instrument Resellers has the capability to support tenants such as Betty's Brass and Sidney's Saxophones as well as Clyde's Clarinets. (See Figure 1.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/transform.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/transform.png?itok=_hc5v4A3" width="1015" height="331" alt="A standalone web application was transformed into a SaaS platform." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A standalone web application was transformed into a SaaS platform. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Implementing SaaS on Kubernetes&lt;/h2&gt; &lt;p&gt;This article describes how to use standard Kubernetes resources—namespaces, deployments, and services—to create different tenants using a common code base. In addition to the standard Kubernetes resources, we use the &lt;a href="https://docs.openshift.com/container-platform/4.10/networking/routes/route-configuration.html"&gt;route resource&lt;/a&gt; provided by &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to create a public URL that enables access to the internal Kubernetes service representing the particular tenant application instance.&lt;/p&gt; &lt;p&gt;The demonstration code runs on the Red Hat OpenShift Container Platform because its route resource provides an easy way to create a domain name that provides access to a tenant running within the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;This article refers to demonstration code for implementing the Instrument Reseller SaaS platform. A subsequent article in this series will describe the code in the demonstration project in detail. For now, you can use the demonstration project as a supporting reference for this article.&lt;/p&gt; &lt;p&gt;Be advised that in order to get the full benefit from reading this article, you need to have an understanding of containers and Kubernetes, particularly around the purpose and use of Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;pods&lt;/a&gt;, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;deployments&lt;/a&gt;, &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Secrets&lt;/a&gt;, and &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;services&lt;/a&gt;. Also, you need to have experience working with the &lt;a href="https://kubernetes.io/docs/reference/kubectl/"&gt;kubectl&lt;/a&gt; client for Kubernetes. In addition, you should be comfortable creating Kubernetes resources using &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/"&gt;manifest (a.k.a. configuration) files&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following sections describe how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use Kubernetes namespaces to isolate tenants in a SaaS platform.&lt;/li&gt; &lt;li&gt;Configure Kubernetes deployments to dedicate application logic to a specific tenant.&lt;/li&gt; &lt;li&gt;Bind a database to a particular tenant using a Kubernetes Secret.&lt;/li&gt; &lt;li&gt;Present a tenant's application logic to the internal network in the cluster using a Kubernetes service.&lt;/li&gt; &lt;li&gt;Expose the tenant outside of the cluster using an OpenShift route.&lt;/li&gt; &lt;li&gt;Deploy and update tenant application logic using a basic &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous deployment (CI/CD)&lt;/a&gt; process.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The role of Kubernetes namespaces in SaaS&lt;/h2&gt; &lt;p&gt;Supporting multiple tenants in a single cluster has been a fundamental feature in Kubernetes since its initial release. Under Kubernetes, it is entirely possible for many tenants to share instances of a common code base while running in isolation from each other.&lt;/p&gt; &lt;p&gt;There are &lt;a href="https://developers.redhat.com/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;several possible approaches&lt;/a&gt; to multitenancy in a SaaS platform under Kubernetes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build tenant isolation right into the logic of a single application.&lt;/li&gt; &lt;li&gt;Run each tenant in its own cluster.&lt;/li&gt; &lt;li&gt;Run each tenant in its own Kubernetes namespace.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Instrument Reseller SaaS platform takes the third approach and uses namespaces to support multiple tenants in a single Kubernetes cluster. This section explains the details of the namespace concept.&lt;/p&gt; &lt;p&gt;Namespaces, as the name implies, create an operational boundary that can be imposed on other resources. For example, you can create a namespace named &lt;code&gt;foo&lt;/code&gt;, and then create other resources such as pods and services under that &lt;code&gt;foo&lt;/code&gt; namespace. Those resources know only about other resources in the &lt;code&gt;foo&lt;/code&gt; namespace. Resources outside of that namespace have no access to resources inside the namespace.&lt;/p&gt; &lt;p&gt;In a multitenant service using namespace isolation, each tenant in the Kubernetes cluster is represented by a particular namespace. The deployment, service, and route resources for the particular tenant are created in that tenant's namespace. Figure 2 illustrates how Kubernetes namespaces isolate tenants in the Instrument Resellers SaaS platform.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/tenants.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/tenants.png?itok=8tDzGWKq" width="385" height="565" alt="Each tenant has its own namespace and URL, but runs the same application." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Each tenant has its own namespace and URL, but runs the same application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Although three tenants are shown in Figure 2, this article shows configurations just for Betty's Brass and Clyde's Clarinets because two tenants are enough to illustrate the concepts you need to know. Table 1 shows the manifest files that declare the Kubernetes namespaces for these tenants. The two manifests are the same except for the &lt;code&gt;name&lt;/code&gt; properties.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 1: Manifests declaring namespaces.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;kind: Namespace apiVersion: v1 metadata: name: bettysbrass labels: name: bettysbrass&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;kind: Namespace apiVersion: v1 metadata: name: clydesclarinets labels: name: clydesclarinets&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;To create each namespace in the Kubernetes cluster, run the following command, where &lt;code&gt;&lt;tenant_namespace&gt;&lt;/code&gt; is the filename of the manifest file particular to the tenant:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f &lt;tenant_namespace&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Creating and configuring tenants within a SaaS using a Kubernetes deployment&lt;/h2&gt; &lt;p&gt;Once the namespaces are created, the next task is to implement the logic for the given tenant according to its assigned namespace. This task uses the Kubernetes deployment resource.&lt;/p&gt; &lt;p&gt;As mentioned previously, a key feature of the Instrument Reseller SaaS is that a single code base can support any number of tenants that want to acquire and resell musical instruments. Application logic for each instrument reseller is represented in the SaaS by a Kubernetes deployment resource.&lt;/p&gt; &lt;p&gt;A deployment controls one or many pod &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;replicas&lt;/a&gt;. The number of pods running under a deployment is determined by the &lt;code&gt;replicas&lt;/code&gt; property in the deployment resource's manifest file.&lt;/p&gt; &lt;p&gt;Therefore, you can change the number of replicas a deployment supports while the application is running. For example, an instrument reseller might start by running three pods. But, after a while, the load on the tenant is such that more pods are needed. To create more pods in the deployment, increase the value assigned to the &lt;code&gt;replicas&lt;/code&gt; property in the manifest file—from three to five, for example. Then re-apply the deployment's manifest file to the cluster. When loads decrease, you can reduce the number of pods in the deployment by changing the &lt;code&gt;replicas&lt;/code&gt; setting in the manifest file and reapplying the changed file to the cluster in the same way.&lt;/p&gt; &lt;p&gt;Should a pod go offline, the deployment resource will create a replacement if possible.&lt;/p&gt; &lt;h3&gt;Customizing deployments&lt;/h3&gt; &lt;p&gt;In our architecture, each deployment should be dedicated to a single instrument reseller. You create the deployment in that instrument reseller's namespace and define the parameters needed by that reseller, such as the URL where it takes orders, through environment variables in the Kubernetes manifest.&lt;/p&gt; &lt;p&gt;For instance, Table 2 shows the manifests that configure the Kubernetes deployment for Betty's Brass and Clyde's Clarinets. The only differences are the values for names and instruments.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 2: Manifests configuring deployments.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: instrumentreseller namespace: bettysbrass labels: app: instrumentreseller spec: replicas: 3 selector: matchLabels: app: instrumentreseller template: metadata: labels: app: instrumentreseller spec: initContainers: - name: seeder image: quay.io/rhdevelopers/instrumentresellerseeder env: - name: RESELLER_DB_NAME value: "brass" - name: RESELLER_INSTRUMENT value: "brass" - name: SEEDER_COUNT value: "10" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url containers: - name: instrumentreseller image: quay.io/rhdevelopers/instrumentreseller env: - name: RESELLER_NAME value: "Betty's Brass" - name: RESELLER_INSTRUMENT value: "brass" - name: RESELLER_DB_NAME value: "brass" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url ports: - containerPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: instrumentreseller namespace: clydesclarinets labels: app: instrumentreseller spec: replicas: 3 selector: matchLabels: app: instrumentreseller template: metadata: labels: app: instrumentreseller spec: initContainers: - name: seeder image: quay.io/rhdevelopers/instrumentresellerseeder env: - name: RESELLER_DB_NAME value: "clarinets" - name: SEEDER_COUNT value: "10" - name: RESELLER_INSTRUMENT value: "clarinet" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url containers: - name: instrumentreseller image: quay.io/rhdevelopers/instrumentreseller env: - name: RESELLER_NAME value: "Clyde's Clarinets" - name: RESELLER_INSTRUMENT value: "clarinet" - name: RESELLER_DB_NAME value: "clarinets" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url ports: - containerPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;A key point to understand about the previous examples is that both tenants are using the same container images. In every tenant, the init container uses the &lt;code&gt;quay.io/rhdevelopers/instrumentresellerseeder&lt;/code&gt; image and the main container uses the &lt;code&gt;quay.io/rhdevelopers/instrumentreseller&lt;/code&gt; image. Remember, an essential principle of multiple tenancy in a SaaS platform is that all tenants use the same code base. Having multiple tenants use the same container images supports this basic principle.&lt;/p&gt; &lt;p&gt;Each tenant in the SaaS platform binds to its own database. That database might exist within the Kubernetes cluster or be an external database service defined by a URL. Often, username and password information needed to access the database will be part of the URL.&lt;/p&gt; &lt;p&gt;Putting username and password information in a cluster is always a risky undertaking. A best practice for making username/password information available to pods in a Kubernetes cluster is to use a Kubernetes resource called a &lt;em&gt;Secret&lt;/em&gt;. We will see how our application passes credentials to the database shortly.&lt;/p&gt; &lt;h3&gt;Data seeding&lt;/h3&gt; &lt;p&gt;As briefly mentioned earlier, the pods in the deployment use &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"&gt;init containers&lt;/a&gt; in addition to standard containers. An init container is a container that runs before the main container. In the case of the Instrument Reseller SaaS, the init container does the work of implementing a special feature of the demonstration code: &lt;em&gt;data seeding.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Because we're not working with real retailers in the demo, we use the init container to seed the tenant instance's database with randomized data that is particular to the instrument type sold by the instrument reseller. The purpose of data seeding in the demo is to provide some initial data to view when the application is used for the first time. Betty's Brass will be seeded with data about brass instruments. Clyde's Clarinets will be seeded with data about clarinets. Sidney's Saxophones will be seeded with data specific to saxophones.&lt;/p&gt; &lt;p&gt;Using the data seeding pattern in containers to prepopulate data for an application opens up the risk of redundant seeding. If one simply runs the init container in each pod replica, the deployment tries to seed data to the data source when each replica starts. Unless a precaution is made, unwarranted data seeding will occur.&lt;/p&gt; &lt;p&gt;Therefore, the data seeder is programmed to go out to the data source and check whether pre-existing seed data exists. If seed data is already in the data source, the seeder exits without adding more data.&lt;/p&gt; &lt;h2&gt;Providing database credentials through Kubernetes Secrets&lt;/h2&gt; &lt;p&gt;Secrets are a Kubernetes resource for providing sensitive information to other resources in a secure manner.&lt;/p&gt; &lt;p&gt;Table 3 shows configurations that declare a Secret named &lt;code&gt;mongo-url&lt;/code&gt; in two different namespaces: one for Betty's Brass and the other for Clyde's Clarinets.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 3: Manifests configuring Secrets.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt; apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: bettysbrass type: Opaque stringData: url: &lt;mongo-url-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt; apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: clydesclarinets type: Opaque stringData: url: &lt;mongo-url-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Note that each Secret is assigned to its respective namespace. The Secret named &lt;code&gt;mongo-url&lt;/code&gt; for Betty's Brass is assigned to the &lt;code&gt;bettysbrass&lt;/code&gt; namespace. The Secret with the same &lt;code&gt;mongo-url&lt;/code&gt; name for Clyde's Clarinets is assigned to the &lt;code&gt;clydesclarinets&lt;/code&gt; namespace. Even though each Secret has the same name, they are distinct because they are assigned to different namespaces. Using the same name among resources is one of the benefits of using namespaces.&lt;/p&gt; &lt;h2&gt;Exposing application logic using a Kubernetes service&lt;/h2&gt; &lt;p&gt;Once the Secret is configured for each tenant, the next step is to create the Kubernetes service that exposes the application logic to the internal Kubernetes network within the SaaS platform. Table 4 shows configurations for the Kubernetes service in Betty's Brass using the &lt;code&gt;bettysbrass&lt;/code&gt; namespace, and for Clyde's Clarinets using the &lt;code&gt;clydesclarinets&lt;/code&gt; namespace. Once again, assigning each service to a different namespace keeps the tenants isolated.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 4: Manifests configuring the services.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Service metadata: name: instrumentreseller namespace: bettysbrass spec: selector: app: instrumentreseller ports: - protocol: TCP port: 8088 targetPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Service metadata: name: instrumentreseller namespace: clydesclarinets spec: selector: app: instrumentreseller ports: - protocol: TCP port: 8088 targetPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Exposing the tenant outside of the cluster using an OpenShift route&lt;/h2&gt; &lt;p&gt;The last configuration step is to create the OpenShift route resource that publishes a domain name to expose the tenant outside of the Kubernetes cluster. The manifests in Table 5 declare the OpenShift routes for Betty's Brass and Clyde's Clarinets. Each manifest uses its tenant's namespace as well as a different host.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 5: Manifests configuring OpenShift routes.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: route.openshift.io/v1 kind: Route metadata: name: instrumentreseller namespace: bettysbrass spec: host: bettysbrass.com port: targetPort: 8088 to: kind: Service name: instrumentreseller&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: route.openshift.io/v1 kind: Route metadata: name: instrumentreseller namespace: clydesclarinets spec: host: clydesclarinets.com port: targetPort: 8088 to: kind: Service name: instrumentreseller&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The route knows which service to bind to through the &lt;code&gt;to&lt;/code&gt; attribute at the bottom of each manifest file.&lt;/p&gt; &lt;p&gt;Declaring a set of manifest files for the Kubernetes namespace, deployment, Secret, service, and route are the first steps to getting a tenant up and running in a Kubernetes cluster. Once the manifest files are created, execute the following command to get each of the tenants running in the Kubernetes cluster, where &lt;code&gt;&lt;manifest_file&gt;&lt;/code&gt; is the name of the manifest file for the given tenant:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f &lt;manifest_file&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Assuming proper configuration, you'll have a tenant up and running using nothing more than a few &lt;code&gt;kubectl&lt;/code&gt; commands. However, as those of us who have spent a lot of time working with Kubernetes have come to understand, the words "proper configuration" can mean hours if not days of labor. In short, wiring everything up is hard. You have to be careful.&lt;/p&gt; &lt;p&gt;So to end this article, we'll devise a deployment process for our SaaS deployment that can be easily automated.&lt;/p&gt; &lt;h2&gt;A CI/CD release process&lt;/h2&gt; &lt;p&gt;Deploying tenants into a SaaS platform comes with varying degrees of complexity. You can do a manual deployment in which you create Linux container images for the SaaS platform's application logic and then push those images out to a container image registry such as &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Then, once the required container images are on the registry, create manifest files that you'll use to realize the Kubernetes deployment resource in the Kubernetes cluster in which the SaaS platform is running. These manifest files declare the application container images that will be used.&lt;/p&gt; &lt;p&gt;Having created the manifest files, run the &lt;code&gt;kubectl apply&lt;/code&gt; command shown near the end of the previous section to create the associated Kubernetes resource in the cluster.&lt;/p&gt; &lt;p&gt;The process just described is shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/manual.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/manual.png?itok=75GdIfGA" width="941" height="115" alt="Manual deployment supports multiple tenants, running a kubectl apply command for us." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Manual deployment supports multiple tenants, running a kubectl apply command for us. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Manual deployment is a feasible way to work with a SaaS platform for research and experimentation. But it's not realistic for today's production releases, which call for automating the process.&lt;/p&gt; &lt;p&gt;Using automation is particularly appropriate for organizations that have a number of teams supporting a SaaS platform. Relying on email and word-of-mouth communication between teams can be risky. Automation helps bring formality to the release process.&lt;/p&gt; &lt;p&gt;Central to release automation is a CI/CD controller such as &lt;a href="https://www.jenkins.io"&gt;Jenkins&lt;/a&gt; or &lt;a href="https://docs.openshift.com/container-platform/4.10/cicd/pipelines/understanding-openshift-pipelines.html"&gt;OpenShift Pipelines&lt;/a&gt;. The CI/CD controller automates many if not all of the tasks necessary to get an application's artifacts from a source code repository into production.&lt;/p&gt; &lt;p&gt;Figure 4 shows an example of a CI/CD process that updates a SaaS platform. The CI/CD controller does the work of packaging up code that's ready for release into a container image. Then it pushes that image to a container registry and updates the SaaS platform with the new version of the image.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ci.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ci.png?itok=Pk4Kvmym" width="1159" height="595" alt="An automated CI/CD process for a multitenant SaaS platform uses a CI/CD controller for several steps." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: An automated CI/CD process for a multitenant SaaS platform uses a CI/CD controller for several steps. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The numbered steps in Figure 4 are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The developer updates the code and commits the updated work to the dev branch in the source code repository.&lt;/li&gt; &lt;li&gt;The quality assurance (Q/A) team escalates code from the dev branch into the Q/A branch and runs unit tests. If the tests pass, Q/A executes integration testing. Upon successful execution, Q/A notifies the release management team that a new version of code is ready for escalation to the main branch of the source code repository.&lt;/li&gt; &lt;li&gt;Release management merges the code into the main branch.&lt;/li&gt; &lt;li&gt;Release management updates the Kubernetes manifest files with the new version tag of the container image associated with the intended release. Updated files are committed to the manifest file repository.&lt;/li&gt; &lt;li&gt;Upon a successful merge of source code and manifest files, the CI/CD controller is notified via automation that the code is ready for packaging into a container image and deployment to a container image registry such as Quay.io.&lt;/li&gt; &lt;li&gt;The CI/CD controller gets the updated code from the source code repository and makes an updated container image from the &lt;a href="https://github.com/containers/common/blob/main/docs/Containerfile.5.md"&gt;Containerfile&lt;/a&gt; stored in the repository along with the application source code.&lt;/li&gt; &lt;li&gt;The CI/CD controller pushes the updated container image to a container image repository.&lt;/li&gt; &lt;li&gt;The CI/CD controller gets the updated manifest files for the relevant tenants from the manifest file repository and runs the &lt;code&gt;kubectl apply&lt;/code&gt; command discussed earlier to update the pods running in the Kubernetes cluster with the container image that has the latest version of the application code.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keep in mind that release processes usually vary among organizations. There is rarely a one-size-fits-all approach to automated releases using a CI/CD controller. This example is one of many possibilities.&lt;/p&gt; &lt;p&gt;The important thing to understand is that when an automated CI/CD process is in place, it handles much of the detailed work of getting code from a release branch into a multitenant Kubernetes cluster in production. Release tasks vary, but in general many details are handled through scripted automation in the CI/CD controller. Release personnel don't fiddle around with manual tasks unless they're facing a mission-critical emergency. Rather, changes in the CI/CD process are implemented by altering automation scripts.&lt;/p&gt; &lt;h2&gt;Kubernetes supports scalable multitenant SaaS&lt;/h2&gt; &lt;p&gt;As this article has shown, hosting a multitenant SaaS platform on Kubernetes can be straightforward. As long as the common code base used by the platform's tenants is generic, implementation involves configuring and deploying the namespace, Secret, deployment, service, and route. All these resources except the route are built into Kubernetes. The route resource is provided by OpenShift.&lt;/p&gt; &lt;p&gt;The application logic common to all tenants is encapsulated into container images that are stored in a container registry. The image is downloaded to the cluster according to configuration information set in the manifest file of the deployment for the given tenant. Finally, production-level releases are automated using a CI/CD controller.&lt;/p&gt; &lt;p&gt;Most SaaS platforms are intended for a particular set of use cases. Each tends to be special. As a result, a platform will have its own set of complexities and exceptions that need to be accommodated. Still, implementing a SaaS platform using Kubernetes is a lot easier than building one from scratch. Kubernetes does most if not all of the heavy listing.&lt;/p&gt; &lt;p&gt;This article covered the fundamental concepts and techniques for implementing a multitenant SaaS platform in a Kubernetes cluster. The next article in this series will take a detailed look at the demonstration application used in this article. That article will describe how to program the generic logic used by all tenants in the demonstration SaaS platform. The article will also describe how to get the demonstration project up and running as a multitenant SaaS platform hosted in a Kubernetes cluster.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/12/implement-multitenant-saas-kubernetes" title="Implement multitenant SaaS on Kubernetes"&gt;Implement multitenant SaaS on Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-08-12T07:00:00Z</dc:date></entry><entry><title type="html">Running Custom Tasks in jBPM With Work Item Handlers</title><link rel="alternate" href="https://blog.kie.org/2022/08/running-custom-tasks-in-jbpm-with-work-item-handlers.html" /><author><name>Helber Belmiro</name></author><id>https://blog.kie.org/2022/08/running-custom-tasks-in-jbpm-with-work-item-handlers.html</id><updated>2022-08-11T12:04:03Z</updated><content type="html">RUNNING CUSTOM TASKS IN JBPM WITH WORK ITEM HANDLERS INTRODUCTION You can use a WorkItemHandler to run custom tasks during the execution of a process in jBPM. In this article, you will run through the steps to create a custom task and use it in a process. &gt; IMPORTANT: This tutorial uses version 7.72.0.Final of jBPM. The application you’re going to create is a process that concatenates first and last names, and prints the result to the console. The concatenation is going to be processed in a custom task. So, start by creating the WorkItemHandler. &gt; NOTE: If you don’t want to create the project, you can clone it from . CREATING THE WORKITEMHANDLER 1. Run the following command to create a work item handler project: mvn archetype:generate \ -DarchetypeGroupId=org.jbpm \ -DarchetypeArtifactId=jbpm-workitems-archetype \ -DarchetypeVersion=7.72.0.Final \ -DgroupId=org.acme \ -DartifactId=myconcatworkitem \ -Dversion=1.0.0-SNAPSHOT \ -DclassPrefix=MyConcat 2. Replace the content of src/main/java/org/acme/MyConcatWorkItemHandler.java file with the following: package org.acme; import org.jbpm.process.workitem.core.AbstractLogOrThrowWorkItemHandler; import org.jbpm.process.workitem.core.util.RequiredParameterValidator; import org.jbpm.process.workitem.core.util.Wid; import org.jbpm.process.workitem.core.util.WidMavenDepends; import org.jbpm.process.workitem.core.util.WidParameter; import org.jbpm.process.workitem.core.util.WidResult; import org.jbpm.process.workitem.core.util.service.WidAction; import org.jbpm.process.workitem.core.util.service.WidAuth; import org.jbpm.process.workitem.core.util.service.WidService; import org.kie.api.runtime.process.WorkItem; import org.kie.api.runtime.process.WorkItemManager; import java.util.HashMap; import java.util.Map; @Wid(widfile = "MyConcatDefinitions.wid", name = "MyConcatDefinitions", displayName = "MyConcatDefinitions", defaultHandler = "mvel: new org.acme.MyConcatWorkItemHandler()", documentation = "myconcatworkitem/index.html", category = "myconcatworkitem", icon = "MyConcatDefinitions.png", parameters = { @WidParameter(name = "FirstName"), @WidParameter(name = "LastName") }, results = { @WidResult(name = "FullName") }, mavenDepends = { @WidMavenDepends(group = "org.acme", artifact = "myconcatworkitem", version = "1.0.0-SNAPSHOT") }, serviceInfo = @WidService(category = "myconcatworkitem", description = "${description}", keywords = "", action = @WidAction(title = "Sample Title"), authinfo = @WidAuth(required = true, params = {"FirstName", "LastName"}, paramsdescription = {"First name", "Last name"}, referencesite = "referenceSiteURL") ) ) public class MyConcatWorkItemHandler extends AbstractLogOrThrowWorkItemHandler { public void executeWorkItem(WorkItem workItem, WorkItemManager manager) { try { RequiredParameterValidator.validate(this.getClass(), workItem); String firstName = (String) workItem.getParameter("FirstName"); // Gets the "FirstName" parameter String lastName = (String) workItem.getParameter("LastName"); // Gets the "LastName" parameter String fullName = firstName + " " + lastName; // Concatenates the "firstName" and "lastName" Map results = new HashMap(); results.put("FullName", fullName); // Adds "fullName" to the "results" object manager.completeWorkItem(workItem.getId(), results); } catch (Throwable cause) { handleException(cause); } } @Override public void abortWorkItem(WorkItem workItem, WorkItemManager manager) { } } 3. Update the src/test/java/org/acme/MyConcatWorkItemHandlerTest.java test file with the following: package org.acme; import org.drools.core.process.instance.impl.WorkItemImpl; import org.jbpm.process.workitem.core.TestWorkItemManager; import org.jbpm.test.AbstractBaseTest; import org.junit.Test; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertNotNull; import static org.junit.Assert.assertTrue; public class MyConcatWorkItemHandlerTest extends AbstractBaseTest { @Test public void testHandler() { WorkItemImpl workItem = new WorkItemImpl(); workItem.setParameter("FirstName", "John"); workItem.setParameter("LastName", "Doe"); TestWorkItemManager manager = new TestWorkItemManager(); MyConcatWorkItemHandler handler = new MyConcatWorkItemHandler(); handler.setLogThrownException(true); handler.executeWorkItem(workItem, manager); assertNotNull(manager.getResults()); assertEquals(1, manager.getResults().size()); assertEquals("John Doe", manager.getResults().get(0L).get("FullName")); assertTrue(manager.getResults().containsKey(workItem.getId())); } } 4. Package and install the work item handler project into your Maven local repository. From the myconcatworkitem directory, run: mvn clean install You should see the generated myconcatworkitem-1.0.0-SNAPSHOT.jar file in the target directory. ADDING THE WORK ITEM HANDLER TO BUSINESS CENTRAL AS A CUSTOM TASK 1. Open Business Central 2. Click the gear icon in the upper-right corner 3. Click "Custom Tasks Administration" 4. Click the "Add Custom Task" button 5. Upload the myconcatworkitem-1.0.0-SNAPSHOT.jar file. After the upload, the Custom Task should appear in the list of Custom Tasks in the same window as the "Add Custom Task" button 6. Locate the Custom Task (MyConcatDefinitions) in the list and activate it INSTALLING THE CUSTOM TASK IN YOUR PROJECT 1. Open your project in Business Central and click "Settings" 2. Click "Custom Tasks" on the left corner 3. Click the "Install" button of the Custom Task (MyConcatDefinitions) 4. Click the "Save" button 5. On the left side of the screen, click "Dependencies" 6. Click "Add from Repository" and then search for the "myconcatworkitem" artifact and select it 7. Click the "Save" button and confirm the dialog USING THE CUSTOM TASK 1. Create a new Business Process 2. Add three process variables to the process. In "Properties", expand "Process Data" and add the following String variables: * firstName * lastName * fullName 3. Add a new Start Event 4. Add a new End Event 5. Click "Custom Tasks" (gear button on the left side of the screen), select "MyConcatDefinitions" and add it to the process 6. Select the node you just added and in Properties, expand "Data Assignments" and click the edit button 7. Bind the process variables to the Custom Task parameters and click OK 8. Add a Script Task to the process 9. Select the node you just added and in Properties, expand "Implementation/Execution" and add the following script: System.out.println(fullName); 10. Connect all the nodes in the process Start - MyConcatDefinitions - Task - End 11. Save and deploy the process When you start a new process instance, you’ll be asked to enter the first and last names. After submitting, you will see the concatenated fullName in the console. CONCLUSION In this tutorial, you’ve learned how to create and use a custom work item handler in jBPM by creating a process that concatenates the first and last names received as parameters. The post appeared first on .</content><dc:creator>Helber Belmiro</dc:creator></entry><entry><title>Connect to services on Kubernetes easily with kube-service-bindings</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/11/connect-services-kubernetes-easily-kube-service-bindings" /><author><name>Costas Papastathis, Michael Dawson</name></author><id>321fb836-2526-4793-b7e5-aab63bfa6520</id><updated>2022-08-11T07:00:00Z</updated><published>2022-08-11T07:00:00Z</published><summary type="html">&lt;p&gt;One of the projects the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; team at Red Hat has been focusing on over the past year is the development of &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. We've found that combining the &lt;a href="https://operatorhub.io/operator/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; and kube-service-bindings is a convenient and consistent way of sharing credentials for services, letting you easily secure your deployments.&lt;/p&gt; &lt;p&gt;This article is the first of a three-part series. Our goals in the series are to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Introduce Kubernetes service bindings and the Service Binding Operator.&lt;/li&gt; &lt;li&gt;Explain how the kube-service-bindings NPM package supports service bindings for Node.js applications.&lt;/li&gt; &lt;li&gt;Cover the clients we've added support for in kube-service-bindings.&lt;/li&gt; &lt;li&gt;Show an end-to-end deployment of a Node.js application communicating with a database in a Kubernetes setting.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After reading this article and parts &lt;a href="https://developers.redhat.com/articles/2022/08/16/enable-backing-services-kubernetes-kube-service-bindings"&gt;two&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/08/16/connect-mongodb-nodejs-application-kube-service-bindings"&gt;three&lt;/a&gt; of the series, you should have a better understanding of how service bindings and kube-service-bindings work. This first article explains the tools we're using and their benefits for Node.js programmers using Kubernetes.&lt;/p&gt; &lt;h2&gt;What are service bindings and the Service Binding Operator?&lt;/h2&gt; &lt;p&gt;In our &lt;a href="https://docs.openshift.com/container-platform/4.9/applications/connecting_applications_to_services/understanding-service-binding-operator.html#service-binding-terminology"&gt;service binding terminology for Kubernetes&lt;/a&gt;, a service binding provides information about a service to a process that needs to bind to that service. In subsequent parts of this series, for instance, you will use a service binding to provide the credentials of a &lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt; database when your Node.js application connects to it. The database is called a &lt;em&gt;backing service&lt;/em&gt;, while the application is called the &lt;em&gt;workload&lt;/em&gt;. The data passed between them is called &lt;em&gt;binding data&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Protocols and rules for sharing information are laid out in the &lt;a href="https://github.com/servicebinding/spec#service-binding-specification-for-kubernetes"&gt;Service Binding Specification for Kubernetes&lt;/a&gt;. The Service Binding Operator (SBO) establishes a connection to share the credentials between the workload and backing service. The SBO is &lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;implemented by Red Hat&lt;/a&gt; and is available in the &lt;a href="https://operatorhub.io/"&gt;OpenShift Operator Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Service Binding Operator has two significant benefits compared to other methods of sharing secrets. The first is security: The SBO requires less exposure of credentials or secrets throughout the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; process. The second benefit is convenience: Rarely is development as easy as dragging a line in a graphical user interface (UI) in the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; console.&lt;/p&gt; &lt;p&gt;You can find more information on how the Service Binding Operator works in the article &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Announcing Service Binding Operator 1.0 GA&lt;/a&gt;. The article &lt;a href="https://developers.redhat.com/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs"&gt;Simplify secure connections to PostgreSQL databases with Node.js&lt;/a&gt; provides information about using the SBO to share credentials among backing services and compares the technique to others, using as an example the PostgreSQL client supported by kube-service-bindings.&lt;/p&gt; &lt;p&gt;In the background, the SBO:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Passes a variable named &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; to the application environment to direct it to the credentials.&lt;/li&gt; &lt;li&gt;Projects the binding data into the application container, under the directory &lt;code&gt;/$SERVICE_BINDING_ROOT/&lt;application-name&gt;&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What is kube-service-bindings and how does it work?&lt;/h2&gt; &lt;p&gt;kube-service-bindings finds, parses, and transforms data such as credentials into a consumable format appropriate for each client, such as a database. kube-service-bindings checks the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable to find which directory in the application instance has the binding data. The presence of the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable indicates that binding data is available. If the environment variable or binding data are not available, kube-service-bindings throws an error, which can easily be discovered through a try/catch block.&lt;/p&gt; &lt;p&gt;Besides parsing the data, kube-service-bindings knows exactly what it takes to provide the right configuration for each client, another advantage to using the package. Our goal in developing kube-service-bindings is to support the most common clients. We started by supporting backing services listed in the General Availability (GA) release for the Red Hat Service Binding Operator, as outlined in the section &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga#extracting_the_binding_data_from_backing_services"&gt;Extracting the binding data from backing services&lt;/a&gt; of the previously mentioned article.&lt;/p&gt; &lt;p&gt;Version 1.0 of kube-service-bindings has made a good start in its support for clients. We would like to prioritize our work based on the needs of the community, so feel free to open a request in &lt;a href="https://github.com/nodeshift/kube-service-bindings/issues"&gt;the kube-service-bindings repository&lt;/a&gt; for the next client you would like to see supported.&lt;/p&gt; &lt;p&gt;Table 1 shows the currently supported clients. To connect to a client, the Node.js program issues a &lt;code&gt;getBinding&lt;/code&gt; call, passing the type (column 1) as the first argument and the client (column 2) as the second.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="NaN"&gt; &lt;caption&gt;Table 1: Clients currently supported by kube-service-bindings.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt; &lt;p&gt;Type&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;Client&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;Date Added&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;AMQP&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/rhea"&gt;rhea&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;March 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Kafka&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/node-rdkafka"&gt;node-rdkafka&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;April 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Kafka&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/kafkajs"&gt;kafkajs&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;April 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MongoDB&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mongodb"&gt;MongoDB&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;February 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MongoDB&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mongoose"&gt;mongoose&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;June 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mysql"&gt;MySQL&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mysql2"&gt;MySQL 2&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/odbc"&gt;odbc&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;PostgreSQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/odbc"&gt;odbc&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;June 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;PostgreSQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/pg"&gt;postgres&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;December 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Redis&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/redis"&gt;redis&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;January 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Redis&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/ioredis"&gt;ioredis&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;January 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;h2&gt;Simplifying access to services on Kubernetes&lt;/h2&gt; &lt;p&gt;This article has explained the roles of the Service Binding Operator and kube-service-bindings in making it easy to connect to backing services such as databases. Subsequent articles in this series will go through an example that connects a Node.js application to a database using these tools.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/11/connect-services-kubernetes-easily-kube-service-bindings" title="Connect to services on Kubernetes easily with kube-service-bindings"&gt;Connect to services on Kubernetes easily with kube-service-bindings&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Costas Papastathis, Michael Dawson</dc:creator><dc:date>2022-08-11T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 11 August 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-08-11.html" /><category term="quarkus" /><category term="kubernetes" /><category term="java" /><category term="jakarta" /><category term="infinispan" /><category term="wildfly" /><category term="cloud-native" /><category term="openshift" /><category term="kogito" /><category term="drools" /><category term="keycloak" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-08-11.html</id><updated>2022-08-11T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kubernetes, java, jakarta, infinispan, wildfly, cloud-native, openshift, kogito, drools, keycloak"&gt; &lt;h1&gt;This Week in JBoss - 11 August 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone! It’s great to be back and bringing you another edition of the JBoss Editorial. As always there’s a lot of exciting news and updates from JBoss communities so let’s dive in.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2022/08/09/infinispan-14"&gt;Infinispan 14.0.0.CR1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/07/keycloak-1901-released.html"&gt;Keycloak 19.0.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-3-3/"&gt;Eclipse Vert.x 4.3.3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/08/02/resteasy-6.1.0-release/"&gt;RESTEasy 6.1.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-11-2-final-released/"&gt;Quarkus 2.11.2.Final&lt;/a&gt; (&lt;code&gt;CVE-2022-2466&lt;/code&gt; is still ongoing)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/08/05/WildFly27-Alpha4-Released/"&gt;WildFly 27 Alpha4&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/08/kogito-1-25-0-released.html"&gt;Kogito 1.25.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_wildfly_maven_plugin_to_create_container_images"&gt;WildFly Maven plugin to create container images&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/08/04/wildfly-maven-docker/"&gt;Use the wildfly-maven-plugin to create a Docker image of your application&lt;/a&gt;, by By Jeff Mesnil&lt;/p&gt; &lt;p&gt;Jeff explains how to use the &lt;code&gt;wildlfy-maven-plugin&lt;/code&gt; and the new WildFly runtime image to build container images. The WildFly Maven plugin, currently in beta with Final planned for WildFly 27, offers a new, and very compelling, architecture to control the full runtime from the application &lt;code&gt;pom.xml&lt;/code&gt;. Developers control the full customization of WildFly using feature packs, packaging scripts, and other artifacts. This approach ensures that the runtime fits the user’s application. Creating a container image is simply a matter of putting it in a runtime image that contains OpenJDK.&lt;/p&gt; &lt;p&gt;The WildFly team are starting an open conversation to bring additional synergies between the Docker and S2I images for WildFly that could benefit the whole community. The team are aiming to bring new capabilities, additional architectures (in particular &lt;code&gt;linux/arm64&lt;/code&gt;), and newer versions of the JDK to all WildFly images. Be sure to check out Jeff’s post and find out how you can get involved!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_jakarta_bean_validation"&gt;Jakarta Bean Validation&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/java-ee/validation/test/"&gt;Getting started with Jakarta Bean Validation&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco takes a look at the Jakarta Bean Validation specification which allows you to express constraints on your model and create custom ones in an extensible way. His detailed post shows you how to write a constraint once and use it in any application layer. Given that Bean validation is layer agnostic, meaning that you can use the same constraint from the presentation to the business model layer.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_kogito_rules_drools_with_java_inheritance"&gt;Kogito Rules (Drools) with Java Inheritance&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/08/kogito-rules-drools-with-java-inheritance.html"&gt;Kogito Rules (Drools) with Java Inheritance&lt;/a&gt;, by Jeff Taylor&lt;/p&gt; &lt;p&gt;In this article, Jeff explains how Kogito rules services can reason over application domain model facts that are represented using plain old Java objects, or POJOs, that use standard Java inheritance. DRL rules files can use POJOs as well as client applications that call the Kogito rules services.&lt;/p&gt; &lt;p&gt;Jeff explores two approaches for sharing Java subclasses between a Kogito rules service and a client application. The first approach isolates objects from each subclass into a JSON array while the second approach uses Jackson inheritance annotations to embed objects from each subclass for REST API calls that serialize and deserialize POJOs to and from JSOn.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_welcome_václav_muzikář"&gt;Welcome Václav Muzikář!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/08/vaclav"&gt;New Keycloak maintainer: Václav Muzikář&lt;/a&gt;, by Bruno Oliveira&lt;/p&gt; &lt;p&gt;The Keycloak team has a new community maintainer! Hearty welcome to Václav Muzikář.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;From unmissable demos to brilliant chat about the latest Java trends, the JBoss community has some great video content for you:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/Urj1X60H6YY"&gt;Quarkus Insights #98: Using Minecraft as an Observability client&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/nH-27gOp0h4"&gt;Quarkus Insights #97: Qute with Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/kdasoBPOWUQ"&gt;Quarkus Insights #96: Quarkus Q&amp;#38;A&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/9DMAkrM_gOA"&gt;MLOps with Flyte with Samhita Alla&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_see_you_next_time"&gt;See you next time&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;em&gt;Hope you enjoyed this edition. Please join us again in two weeks for our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Don Naro</dc:creator></entry></feed>
